# ğŸ” 3DÂ +Â 2DÂ ObjectÂ DetectionÂ &Â Tracking on the KITTI Dataset

ğŸš— **Fusionâ€‘based perception pipeline** that combines **YOLOv8 2â€‘D detections** with **3â€‘D LiDAR clustering**. The project projects LiDAR into camera space, matches 2â€‘D andÂ 3â€‘D detections, overlays both on every KITTI image frame, and renders a full video.

---

## âœ¨Â Features

| Â âœ”Â    | Â FeatureÂ                                         |
| ----- | ------------------------------------------------ |
| Â âœ…Â    | YOLOv8Â 2â€‘D object detectionÂ                      |
| Â âœ…Â    | DBSCAN clustering of 3â€‘D point cloudsÂ            |
| Â âœ…Â    | LiDARÂ â†’Â camera projection via KITTI calibrationÂ  |
| Â âœ…Â    | 2â€‘D/3â€‘D matching &Â falseâ€‘positive filteringÂ      |
| Â âœ…Â    | Frame exportÂ + MP4 video generationÂ              |
| Â ğŸ› ï¸Â  | Hooks for Kalman tracking, IoU matching, etc.Â    |

---

## ğŸ“Â RepositoryÂ Layout

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 2011_09_26_calib/           # KITTI calibration files
â”‚   â”œâ”€â”€ image_02/data/             # KITTI RGB framesÂ (.png)
â”‚   â””â”€â”€ velodyne_points/data/      # KITTI LiDAR scans (.bin)
â”‚
â”œâ”€â”€ projected_frames/              # Autoâ€‘generated overlays (after running)
â”œâ”€â”€ detections_output.mp4          # Final video (autoâ€‘generated)
â”‚
â”œâ”€â”€ run_yolo_on_images.py          # YOLO inference on all frames
â”œâ”€â”€ 2D_3D_detection.py             # Fusion + video script (main)
â”œâ”€â”€ detections_yolo.csv            # YOLO results (autoâ€‘generated)
â”œâ”€â”€ requirements.txt               # Python deps
â””â”€â”€ README.md                      # This file
```

---

## âš™ï¸Â Requirements

```bash
python3 â€‘m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

**Key libraries**

```
ultralytics
opencvâ€‘python
open3d
numpy
pandas
scikitâ€‘learn
tqdm
```

---

## ğŸš€Â QuickÂ Start

\###Â 1ï¸âƒ£Â Run YOLOv8 on KITTI images

```bash
python run_yolo_on_images.py \
       --img-dir data/2011_09_26_drive_0001_sync/image_02/data \
       --model yolov8n.pt        # use s/m/l for higher accuracy
# âœ outputs detections_yolo.csv
```

\###Â 2ï¸âƒ£Â Fuse 2â€‘D &Â 3â€‘D and build video

```bash
python 2D_3D_detection.py \
       --calib data/2011_09_26_calib/2011_09_26 \
       --velodyne data/â€¦/velodyne_points/data \
       --images   data/â€¦/image_02/data \
       --yolo     detections_yolo.csv
# âœ overlays saved to projected_frames/ & detections_output.mp4
```

> **Tip:** change `--fps` inside the script to speed up or slow down playback.

---

## ğŸ¥Â SampleÂ Frame



- GreenÂ = YOLO 2â€‘D box
- Blue dotsÂ = LiDAR points validated inside that box

---

## ğŸ”„Â Roadmap / ComingÂ Soon

- Kalman filter tracking for persistent IDs
- True 3â€‘D bounding boxes projected into image view
- Confidence fusion (2â€‘D/3â€‘D voting)
- ROS bag export

---

## ğŸ“šÂ Dataset

- **KITTI RawÂ 2011â€‘09â€‘26Â DriveÂ 0001**
- 1392Ã—512 stereo images @Â 10Â Hz
- 3â€‘D Velodyne scans (\~100â€‰k pts /Â frame)

---

## ğŸ™Â Acknowledgements

- [UltralyticsÂ YOLOv8](https://github.com/ultralytics/ultralytics)
- [Open3D](https://github.com/isl-org/Open3D)
- [KITTI VisionÂ Benchmark](http://www.cvlibs.net/datasets/kitti/)

---

## ğŸ“„Â License

ApacheÂ 2.0 â€” free for research &Â education. Please cite appropriately if you use this code.

